---
title: "Predict the status of #TidyTuesday Bigfoot sightings"
author: Julia Silge
date: '2022-09-23'
format: hugo
slug: bigfoot
categories:
  - rstats
  - tidymodels
tags:
  - rstats
  - tidymodels
summary: "Learn how to use vetiver to set up different types of prediction endpoints for your deployed model."
---

```{r setup}
#| include: false
library(knitr)
knitr::opts_chunk$set(cache = TRUE, cache.lazy = FALSE, warning = FALSE, 
                      message = FALSE, echo = TRUE, dpi = 180,
                      fig.width = 7, fig.height = 5)
library(tidyverse)
library(silgelib)
theme_set(theme_plex())
update_geom_defaults("rect", list(fill = "midnightblue", alpha = 0.8))
update_geom_defaults("line", list(color = "midnightblue", alpha = 0.8))

library(wikipediapreview)
wp_init()

## if you don't have fancy fonts like IBM Plex installed, run
## theme_set(theme_minimal())
```

This is the latest in my series of [screencasts](https://www.youtube.com/juliasilge)! I am now working on [MLOps tooling](https://vetiver.rstudio.com/) full-time, and this screencast shows how to use vetiver to set up different types of prediction endpoints, using this week's [`#TidyTuesday` dataset](https://github.com/rfordatascience/tidytuesday) on Bigfoot sightings. ðŸ¦¶

```{r}
#| echo: false
blogdown::shortcode("youtube", "XM_p0XpLd5s")
```

</br>

Here is the code I used in the video, for those who prefer reading instead of or in addition to video.

## Explore data

Our modeling goal is to predict the classification of a [Bigfoot report](https://github.com/rfordatascience/tidytuesday/tree/master/data/2022/2022-09-13) based on the text used in the report. Let's start by reading in the data:

```{r}
library(tidyverse)
bigfoot_raw <- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-09-13/bigfoot.csv')

bigfoot_raw %>%
  count(classification)
```

This dataset [categorizes the reports into three classes](http://www.bfro.net/GDB/classify.asp). Class A is for clear visual sightings, Class B is for reports without a clear visual identification, and Class C is for second-hand or otherwise less reliable reports. There are very few Class C reports, so let's only focus on Classes A and B.

```{r}
bigfoot <-
  bigfoot_raw %>%
  filter(classification != "Class C", !is.na(observed)) %>%
  mutate(
    classification = case_when(
      classification == "Class A" ~ "sighting",
      classification == "Class B" ~ "possible"
    )
  )

bigfoot
```

What words from the report have the highest log odds of coming from either category?

```{r}
library(tidytext)
library(tidylo)

bigfoot %>%
  unnest_tokens(word, observed) %>%
  count(classification, word) %>%
  filter(n > 100) %>%
  bind_log_odds(classification, word, n) %>%
  arrange(-log_odds_weighted)
```

When someone has made a sighting, they see or witness a furry ape, maybe in their headlights. The reports without a clear visual sighting definitely seem like they are about sound, hearing screams and howls.

## Build a model

We can start by loading the tidymodels metapackage, splitting our data into training and testing sets, and creating cross-validation samples. Think about this stage as *spending your data budget*.

```{r}
library(tidymodels)

set.seed(123)
bigfoot_split <-
  bigfoot %>%
  select(observed, classification) %>%
  initial_split(strata = classification)

bigfoot_train <- training(bigfoot_split)
bigfoot_test <- testing(bigfoot_split)

set.seed(234)
bigfoot_folds <- vfold_cv(bigfoot_train, strata = classification)
bigfoot_folds
```

Next, let's create our feature engineering recipe using word tokenization. This dataset (compared to [for example modeling LEGO set names](https://juliasilge.com/blog/lego-sets/)) involves much longer documents with a larger vocabulary. It is more what we would call "natural language" with English speakers using their vocabularies in typical ways, so let's keep a pretty large number of tokens.

```{r}
library(textrecipes)

bigfoot_rec <-
  recipe(classification ~ observed, data = bigfoot_train) %>%
  step_tokenize(observed) %>%
  step_tokenfilter(observed, max_tokens = 2e3) %>%
  step_tfidf(observed)

bigfoot_rec
```

Next let's create a model specification for a lasso regularized logistic regression model. Lasso models can be a good choice for text data when the feature space (number of unique tokens) is big with lots of predictors. We can combine this together with the recipe in a workflow:

```{r}
glmnet_spec <- 
  logistic_reg(mixture = 1, penalty = tune()) %>%
  set_engine("glmnet")

bigfoot_wf <- workflow(bigfoot_rec, glmnet_spec)
```

We don't know the right amount of regularization (`penalty`) for this model, so we let's tune over possible penalty values with our resamples.

```{r}
doParallel::registerDoParallel()
set.seed(123)
bigfoot_res <- 
  tune_grid(
    bigfoot_wf, 
    bigfoot_folds, 
    grid = tibble(penalty = 10 ^ seq(-3, 0, by = 0.3))
  )

autoplot(bigfoot_res)
```

We can identify the numerically best amount of regularization:

```{r}
show_best(bigfoot_res)
```

In a case like this, we might want to choose a simpler model, i.e. a model with more regularization and fewer features (words) in it. We can identify the simplest model configuration that has performance within a certain percent loss of the numerically best one:

```{r}
select_by_pct_loss(bigfoot_res, desc(penalty), metric = "roc_auc")
```

Now let's finalize our original tuneable workflow with this penalty value, and then **fit** one time to the training data and **evaluate** one time on the testing data.

```{r}
bigfoot_final <-
  bigfoot_wf %>%
  finalize_workflow(
    select_by_pct_loss(bigfoot_res, desc(penalty), metric = "roc_auc")
  ) %>%
  last_fit(bigfoot_split)

bigfoot_final
```

How did this final model do, evaluated using the testing set?

```{r}
collect_metrics(bigfoot_final)
```

We can see the model's performance across the classes using a confusion matrix.

```{r}
collect_predictions(bigfoot_final) %>%
  conf_mat(classification, .pred_class)
```

Let's also check out the variables that ended up most important after regularization.

```{r}
library(vip)
bigfoot_final %>%
  extract_fit_engine() %>%
  vi() 
```

Here again, we see the words about seeing vs. hearing Bigfoot.

## Deploy the model

The first step to deploy this model is to create a deployable model object with [vetiver](https://vetiver.rstudio.com/).

```{r}
library(vetiver)
v <- bigfoot_final %>%
  extract_workflow() %>%
  vetiver_model("bigfoot")
v
```

The typical next steps is to [version](https://vetiver.rstudio.com/get-started/version.html) your model, but for this blog post, let's go straight to how we would make predictions. We can use `predict()` or `augment()` with our deployable model object:

```{r}
augment(v, slice_sample(bigfoot_test, n = 10))
```

To deploy this model elsewhere away from my local machine, I would set up a REST API. You can specify **what kind** of predictions to make when you create that API. I can set up an API to make the default predictions (`type = "class"` would be the default in this case for tidymodels):

```{r}
library(plumber)
## pipe this to `pr_run()`
pr() %>% 
  vetiver_api(v)
```

If I want to specify an argument to control the type of predictions, like returning probabilities instead of the predicted class, I can do that instead:

```{r}
## pipe this to `pr_run()`
pr() %>% 
  vetiver_api(v, type = "prob")
```

We would generally recommend that these kinds of decisions are made at the API or endpoint level, e.g. set up a single endpoint for a single type of prediction. However, you could write custom code so arguments like the `type` of prediction are passed in as query parameters if you wish.

