---
title: "Use OpenAI text embeddings with #TidyTuesday horror movie descriptions"
author: Julia Silge
date: '2023-04-05'
format: hugo
slug: horror-embeddings
categories:
  - rstats
tags:
  - rstats
summary: "High quality text embeddings are becoming more available from companies like OpenAI. Learn how to obtain them and then use them for text analysis."
---

```{r setup}
#| include: false
library(knitr)
knitr::opts_chunk$set(cache = TRUE, cache.lazy = FALSE, warning = FALSE, 
                      message = FALSE, echo = TRUE, dpi = 180,
                      fig.width = 7, fig.height = 5)
library(tidyverse)
library(silgelib)
theme_set(theme_plex())
update_geom_defaults("rect", list(fill = "midnightblue", alpha = 0.8))
update_geom_defaults("line", list(color = "midnightblue", alpha = 0.8))

library(wikipediapreview)
wp_init()

## if you don't have fancy fonts like IBM Plex installed, run
## theme_set(theme_minimal())
```

This is my latest [screencast](https://www.youtube.com/juliasilge), on a topic outside of my typical ML/MLOps subject areas. This screencast walks through how to obtain and use text embeddings from OpenAI with R, with a recent [`#TidyTuesday` dataset](https://github.com/rfordatascience/tidytuesday) on horror movies. Ô∏èüëª

```{r}
#| echo: false
blogdown::shortcode("youtube", "UsaZV8ROMSc")
```

</br>

Here is the code I used in the video, for those who prefer reading instead of or in addition to video.

## Explore data

When [this dataset](https://github.com/rfordatascience/tidytuesday/tree/master/data/2022/2022-11-01) was first shared for Tidy Tuesday back in November, I [computed a correlation network](https://gist.github.com/juliasilge/2708e8b4c9f20a3308afe9101c06ab4d) showing what words are used to describe horror movies. Click through to the gist to see details, but there are clusters of correlated words about groups of high school students, small towns, serial killers, and haunted houses. Let's keep this blog post short, only take a subsample of the movies, and just do a quick `glimpse()` of the data before we get started with text embeddings.

```{r}
library(tidyverse)

set.seed(123)
horror_movies <- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-11-01/horror_movies.csv') %>%
  filter(!is.na(overview), original_language == "en") %>%
  slice_sample(n = 1000)

glimpse(horror_movies)
```

What do these horror movie descriptions look like?

```{r}
set.seed(234)
sample(horror_movies$overview, size = 3)
```

## Text embeddings

I've written about [text embeddings](https://juliasilge.com/blog/tidy-word-vectors/) [before](https://juliasilge.com/blog/word-vectors-take-two/), as well as [given talks about them](https://www.youtube.com/watch?v=ke03DGvT8uU). I would especially refer you to the [whole chapter](https://smltar.com/embeddings.html) in my book with Emil Hvitfeldt on this topic for an introduction and more details. To learn text embeddings, you need a large amount of text data; companies like [OpenAI](https://openai.com/) (known for GPT-3 and GPT-4) are starting to make [high quality embeddings](https://platform.openai.com/docs/guides/embeddings) available. In the case of OpenAI, the embeddings are available for a fee via API. I registered for an API key and then called the API with my horror movie descriptions.

Before we work with text embeddings, it's good to reflect on the biases that are literally encoded into the numbers that we will be dealing with. Whatever human prejudice or bias exists in the corpus used for training becomes imprinted into the vector data of the embeddings. [OpenAI themselves say](https://platform.openai.com/docs/guides/embeddings/limitations-risks):

> The models encode social biases, e.g. via stereotypes or negative sentiment towards certain groups.

Click through to read about the specific evidence of bias in these embeddings, and keep these caveats in mind as we move forward, even with this kind of silly example of horror movie descriptions.

```{r}
library(httr)
embeddings_url <- "https://api.openai.com/v1/embeddings"
auth <- add_headers(Authorization = paste("Bearer", Sys.getenv("OPENAI_API_KEY")))
body <- list(model = "text-embedding-ada-002", input = horror_movies$overview)

resp <- POST(
  embeddings_url,
  auth,
  body = body,
  encode = "json"
)

embeddings <- content(resp, as = "text", encoding = "UTF-8") %>%
  jsonlite::fromJSON(flatten = TRUE) %>%
  pluck("data", "embedding")
```

This API call costs a couple of cents, as best as I can tell. If you don't want to call the OpenAI API directly, you can use the [openai R package](https://irudnyts.github.io/openai/). Let's add these embeddings as a new column with the horror movie data.

```{r}
horror_embeddings <-
  horror_movies %>%
  mutate(embeddings = embeddings)

horror_embeddings %>%
  select(id, original_title, embeddings)
```

The `"text-embedding-ada-002"` vectors returned from OpenAI are pretty big for text vectors, of length 1536. Think of this as a high dimensional space learned from whatever huge datasets of text that OpenAI uses; each of our horror movie descriptions is located somewhere in the high dimensional space, and horror movies that are described similarly are closer to each while those that are described differently are further away. There is a ton we can now do with these vector representations, like clustering them, or maybe [using the clusters like topic models](https://aclanthology.org/2020.emnlp-main.135). Let's walk through two possible applications: finding similar texts and principal component analysis.

For both, it will be useful to have our embeddings in a matrix, instead of a list of numeric vectors:

```{r}
embeddings_mat <- matrix(
  unlist(horror_embeddings$embeddings), 
  ncol = 1536, byrow = TRUE
)

dim(embeddings_mat)
```

Notice that we have `r scales::comma(nrow(horror_movies))` rows, one for each of the movies, and 1,536 columns for the 1536-dimensional OpenAI text embeddings.

## Similarity

Let's start by finding the texts most similar to a text we are interested in. We can compute a cosine similarity matrix for all the horror movie descriptions:

```{r}
embeddings_similarity <- embeddings_mat / sqrt(rowSums(embeddings_mat * embeddings_mat))
embeddings_similarity <- embeddings_similarity %*% t(embeddings_similarity)
dim(embeddings_similarity)
```

This contains the similarity scores for each description compared to each other description. Let's say we are most interesting in this particular movie:

```{r}
horror_movies %>% 
  slice(4) %>% 
  select(title, overview)
```

Sharktopus!!! ü¶àüêô

Let's pull out the similarity scores relative to this movie:

```{r}
enframe(embeddings_similarity[4,], name = "movie", value = "similarity") %>%
  arrange(-similarity)
```

What are these most similar movies?

```{r}
horror_movies %>% 
  slice(c(935, 379, 380)) %>% 
  select(title, overview)
```

Looks pretty close to me!

## PCA

I love principal component analysis (PCA) and have [written and spoken](https://juliasilge.com/blog/stack-overflow-pca/) about it a fair amount. We can use PCA to transform our high dimensional text embeddings to a new ‚ú®special‚ú® high dimensional space with special characteristics. It probably won't help us to look at what contributes to the first couple of principal component vectors (since we don't know what the text embedding elements mean) but we can project down to the first couple of principal components and see what kinds of movies are most similar or dissimilar. Time for PCA!

```{r}
set.seed(234)
horror_pca <- irlba::prcomp_irlba(embeddings_mat, n = 32)
```

The `horror_pca` object has the standard deviation of the principal components, the matrix of eigenvectors, and in `x`, the original data multiplied by that matrix, i.e. projected in the new space. Let's bind `x` together with our original dataframe so we have the title and other information. When we plot PC1 vs. PC2, we are looking at the components that explain the most difference between movie descriptions.

```{r}
augmented_pca <- 
  as_tibble(horror_pca$x) %>%
  bind_cols(horror_movies)

augmented_pca %>%
  ggplot(aes(PC1, PC2, color = vote_average)) +
  geom_point(size = 1.3, alpha = 0.8) +
  scale_color_viridis_c()
```

Looks to me like the vote average is not related to the movie descriptions, really at all!

Check out an [interactive version of this plot](https://rpubs.com/juliasilge/horror-embeddings) where you can see the titles via tooltip.  Our friend *Sharktopus* is down in the lower left with *Ebola Rex Versus Murder Hornets* while on the opposite upper right are movies like *One Night Stand* and *Sexual Magic*. 
