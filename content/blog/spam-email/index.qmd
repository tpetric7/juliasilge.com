---
title: "Evaluate multiple modeling approaches for #TidyTuesday spam email"
author: Julia Silge
date: '2023-09-01'
format: hugo
slug: spam-email
categories:
  - rstats
  - tidymodels
tags:
  - rstats
  - tidymodels
summary: "Use workflowsets to evaluate multiple possible models to predict whether email is spam."
---

```{r setup}
#| include: false
library(knitr)
knitr::opts_chunk$set(cache = TRUE, cache.lazy = FALSE, warning = FALSE, 
                      message = FALSE, echo = TRUE, dpi = 180,
                      fig.width = 7, fig.height = 5)
library(tidyverse)
library(silgelib)
theme_set(theme_light_plex())
update_geom_defaults("rect", list(fill = "midnightblue", alpha = 0.8))
update_geom_defaults("line", list(color = "midnightblue", alpha = 0.8))

library(wikipediapreview)
wp_init()

## if you don't have fancy fonts like IBM Plex installed, run
## theme_set(theme_minimal())
```

This is the latest in my series of [screencasts](https://www.youtube.com/juliasilge)! This screencast focuses on how to evaluate multiple possible models via [workflowsets](https://workflowsets.tidymodels.org/), using this week's [`#TidyTuesday` dataset](https://github.com/rfordatascience/tidytuesday) on spam email. üìß

```{r}
#| echo: false
blogdown::shortcode("youtube", "5LvTiy9dqrI")
```

</br>

Here is the code I used in the video, for those who prefer reading instead of or in addition to video.

## Explore data

Our modeling goal is to predict which [emails are spam](https://github.com/rfordatascience/tidytuesday/blob/master/data/2023/2023-08-15/readme.md), based on some text features that have been prepared ahead of time. Let's start by reading in the data:

```{r}
library(tidyverse)
spam <- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-08-15/spam.csv')
glimpse(spam)
```

The `yesno` variable tells us whether the email is spam or not, and we have information on the total length of UNINTERRUPTED CAPITAL LETTERS as well as occurrences of $, !, the word "money", the string "000", and the word "make" (all as proportions of characters or words). Notice that a lot of feature engineering has gone into preparing the dataset; we don't have the raw email contents but instead a set of features engineered from that text. This means that we don't need to think much about feature engineering for our example here.

How is the number of UNINTERRUPTED CAPITAL LETTERS distributed?

```{r}
spam |> 
  ggplot(aes(crl.tot, fill = yesno, color = yesno)) +
  geom_density(linewidth = 1.2, alpha = 0.2) +
  scale_x_log10() +
  labs(fill = "Spam?", color = "Spam?")
```

How about those other features? Since those proportions have lots of zeroes, let's compare zero with greater than zero visually:

```{r}
#| fig-width: 10
#| fig-height: 7
spam |> 
  pivot_longer(dollar:make) |> 
  mutate(
    yesno = if_else(yesno == "n", "Not spam", "Spam"),
    value = if_else(value > 0, "Greater than zero", "Zero")
  ) |> 
  ggplot(aes(value, fill = yesno)) +
  geom_bar(alpha = 0.8) +
  facet_wrap(vars(name)) +
  theme(legend.position="bottom") +
  labs(fill = NULL, x = NULL)
```

These features look like they exhibit some _big_ differences between spam and not-spam email.

## Build and compare models

We can start by loading the tidymodels metapackage, splitting our data into training and testing sets, and creating cross-validation resamples. Think about this stage as *spending your data budget*.

```{r}
library(tidymodels)

set.seed(123)
spam_split <-
  spam |> 
  mutate(yesno = as.factor(yesno)) |> 
  initial_split(strata = yesno)

spam_train <- training(spam_split)
spam_test <- testing(spam_split)
set.seed(234)
spam_folds <- vfold_cv(spam_train, strata = yesno)
spam_folds
```

Let's say that we don't know what kind of modeling approach might work best with this data, as is often the case with a new modeling project, so we want to try out several options. Let's start with a naive Bayes classifier, which incidentally is one of the [real ways spam email has been identified](https://en.wikipedia.org/wiki/Naive_Bayes_spam_filtering) (although in a real application, naive Bayes is used with the whole email text contents, not features like these). Let's also include a MARS model and random forest model. Each of these has hyperparameters, so let's include both a model specification where we tune a hyperparameter or two together with one that uses model defaults.

```{r}
library(discrim)

nb_spec <- naive_Bayes()
nb_spec_tune <- naive_Bayes(smoothness = tune())
mars_spec <- mars() |> 
  set_mode("classification")
mars_spec_tune <- mars(num_terms = tune()) |> 
  set_mode("classification")
rf_spec <- rand_forest(trees = 1e3) |> 
  set_mode("classification")
rf_spec_tune <- rand_forest(trees = 1e3, mtry = tune(), min_n = tune()) |> 
  set_mode("classification")
```

Now we can put all these together in a [workflowset](https://workflowsets.tidymodels.org/).

```{r}
spam_models <-
  workflow_set(
    preproc = list(formula = yesno ~ .),
    models = list(
      nb = nb_spec, 
      mars = mars_spec, 
      rf = rf_spec,
      nb_tune = nb_spec_tune, 
      mars_tune = mars_spec_tune, 
      rf_tune = rf_spec_tune
    )
  )

spam_models
```

Since half of these model specifications have tuning parameters, let's use `tune_grid()` to evaluate how each performs; for the models without any tuning parameters, it will automatically use `fit_resamples()` instead. I want to see how the model does for both spam and not spam, so I'll add some specific metrics as well.

```{r}
set.seed(123)
doParallel::registerDoParallel()

spam_res <-
    spam_models |> 
    workflow_map(
        "tune_grid",
        resamples = spam_folds,
        metrics = metric_set(accuracy, sensitivity, specificity)
    )
```

How did all these models do?

```{r}
#| fig-width: 10
#| fig-height: 3.5
autoplot(spam_res)
```

All of the models do better identifying the positive class (`"n"`) than the negative class (`"y"`) but the naive Bayes classifier does even worse than the other two; it does the worst job at identifying real spam email as spam. (Remember that this isn't the same situation as real spam classifiers, which deal with the whole text.) That random forest is looking pretty good!

```{r}
rank_results(spam_res, rank_metric = "accuracy")
```

It turns out that it is the random forest _without tuning_ that is the absolute best, although there is not much difference in performance between the different random forest configurations. This is a great example of why we love random forest and it is used so much! Often turns out great and is low-maintenace about hyperparameters.

## Train and evaluate final model

Let's go with that no-tuning random forest as our final choice for predicting spam from these features. Let's also update the model specification so it computes feature importance during training.

```{r}
spam_wf <- workflow(
    yesno ~ ., 
    rf_spec |> set_engine("ranger", importance = "impurity")
)
spam_fit <- last_fit(spam_wf, spam_split)
spam_fit
```

What does the confusion matrix look like?

```{r}
collect_predictions(spam_fit) |> 
    conf_mat(yesno, .pred_class)
```

It's definitely easier to identify email that is not spam than is spam, but remember that this model did better at that than the other ones we tried. What about an ROC curve?

```{r}
collect_predictions(spam_fit) |> 
    roc_curve(yesno, .pred_n) |> 
    autoplot()
```

Since we updated the random forest model specification to compute variable importance, we can see those results now:

```{r}
library(vip)

extract_workflow(spam_fit) |>
  extract_fit_parsnip() |>
  vip()
```

When we have lots of ‚ÄºÔ∏è‚ÄºÔ∏è‚ÄºÔ∏è‚ÄºÔ∏èan email has a high likelihood of being spam.

## Create a deployable model object

If you wanted to deploy this model, the next step is to create a deployable model object with [vetiver](https://vetiver.rstudio.com/):

```{r}
library(vetiver)

v <- extract_workflow(spam_fit) |> 
    vetiver_model("spam-email-rf")
v
```

Check out the video for what it looks like to run this API locally.

```{r}
#| eval: false
library(plumber)
pr() |> 
    vetiver_api(v) |> 
    pr_run()
```

I am teaching a workshop at [posit::conf()](https://pos.it/conf) in just a couple weeks on how to deploy and maintain models with vetiver, and there are just a few spots left! Join me if you are interested in learning more about this part of the modeling process, or [check out all the other workshops](https://www.youtube.com/playlist?list=PL9HYL-VRX0oROlETlHRDAt0FzqnfkoG84) being put together for later this month.
