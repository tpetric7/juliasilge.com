---
title: 'Topic modeling for #TidyTuesday Taylor Swift lyrics'
author: Julia Silge
date: '2023-10-23'
format: hugo
slug: taylor-swift
categories:
  - rstats
tags:
  - rstats
summary: "Learn how to fit and interpret an unsupervised text model for all of Taylor Swift's ERAS."
---

```{r setup}
#| include: false
library(knitr)
knitr::opts_chunk$set(cache = TRUE, cache.lazy = FALSE, warning = FALSE, 
                      message = FALSE, echo = TRUE, dpi = 180,
                      fig.width = 7, fig.height = 5)
library(tidyverse)
library(silgelib)
theme_set(theme_light_plex())
update_geom_defaults("rect", list(fill = "midnightblue", alpha = 0.8))
update_geom_defaults("line", list(color = "midnightblue", alpha = 0.8))

## if you don't have fancy fonts like IBM Plex installed, run
## theme_set(theme_minimal())
```

This is the latest in my series of [screencasts](https://www.youtube.com/juliasilge)! I saw Taylor Swift's Eras Tour movie over the weekend, and this screencast focuses on unsupervised modeling for text with this week's [`#TidyTuesday` dataset](https://github.com/rfordatascience/tidytuesday) on the songs of Taylor Swift. Today's screencast walks through how to build a [structural topic model](https://www.structuraltopicmodel.com/) and then how to understand and interpret it. ðŸ’–

```{r}
#| echo: false
blogdown::shortcode("youtube", "rXDv0ZuX0Fc")
```

</br>

Here is the code I used in the video, for those who prefer reading instead of or in addition to video.

## Explore data

Our modeling goal is to "discover" topics in the [lyrics of Taylor Swift songs](https://github.com/rfordatascience/tidytuesday/blob/master/data/2023/2023-10-17/readme.md). Instead of a supervised or predictive model where our observations have labels, this is an unsupervised approach.

```{r}
library(tidyverse)
library(taylor)
glimpse(taylor_album_songs)
```

Notice that the `lyrics` variable contains nested tibbles with the texts of the songs; we'll need to unnest these:

```{r}
library(tidytext)

tidy_taylor <-
    taylor_album_songs |>
    unnest(lyrics) |> 
    unnest_tokens(word, lyric)

tidy_taylor
```

We can find the most common words, or see which words are used the most per song:

```{r}
tidy_taylor |> 
    anti_join(get_stopwords()) |> 
    count(track_name, word, sort = TRUE)
```


## Train a topic model

To train a topic model with the stm package, we need to create a sparse matrix from our tidy tibble of tokens. Let's treat each Taylor Swift song as a document, and throw out words used three or fewer times in a song.

```{r}
lyrics_sparse <-
    tidy_taylor |> 
    count(track_name, word) |> 
    filter(n > 3) |> 
    cast_sparse(track_name, word, n)

dim(lyrics_sparse)
```


This means there are 191 song (i.e. documents) and `r ncol(lyrics_sparse)` different tokens (i.e. terms or words) in our dataset for modeling. Notice that I did _not_ remove stop words here. You [typically don't want to remove stop words before building topic models](http://dx.doi.org/10.1162/tacl_a_00099) but we will need to keep in mind that the highest probability words will look mostly the same from each topic.

A topic model like this one models:

- each **document** as a mixture of topics
- each **topic** as a mixture of words

The most important parameter when training a topic modeling is `K`, the number of topics. This is like `k` in k-means in that it is a hyperparamter of the model and we must choose this value ahead of time. We could [try multiple different values](https://juliasilge.com/blog/evaluating-stm/) to find the best value for `K`, but since this is Taylor Swift, let's use `K = 13`.

```{r}
library(stm)
set.seed(123)
topic_model <- stm(lyrics_sparse, K = 13, verbose = FALSE)
```

To get a quick view of the results, we can use `summary()`.

```{r}
summary(topic_model)
```

Notice that we do in fact have fairly uninteresting and common words as the most common for _all_ the topics. This is because we did not remove stopwords.

## Explore topic model results

To explore more deeply, we can `tidy()` the topic model results to get a dataframe that we can compute on. If we did `tidy(topic_model)` that would give us the matrix of topic-word probabilities, i.e. the highest probability words from each topic. This is the boring one that is mostly common words like "you" and "me".

We can alternatively use other metrics for identifying important words, like FREX (high frequency _and_ high exclusivity) or lift:

```{r}
tidy(topic_model, matrix = "lift")
```

This returns a ranked set of words (not the underlying metrics themselves) and gives us a much clearer idea of what makes each topic unique! Topic 1 looks to be more from the *Red* album.

We also can use `tidy()` to get the matrix of document-topic probabilities. For this, we need to pass in the `document_names`:

```{r}
lyrics_gamma <- tidy(
  topic_model, 
  matrix = "gamma",
  document_names = rownames(lyrics_sparse)
) 
```

How are these topics related to Taylor Swift's eras (i.e. albums)?

```{r}
#| fig-width: 10
#| fig-height: 10
lyrics_gamma |> 
    left_join(
        taylor_album_songs |> 
            select(album_name, document = track_name) |> 
            mutate(album_name = fct_inorder(album_name))
    ) |> 
    mutate(topic = factor(topic)) |> 
    ggplot(aes(gamma, topic, fill = topic)) +
    geom_boxplot(alpha = 0.7, show.legend = FALSE) +
    facet_wrap(vars(album_name)) +
    labs(x = expression(gamma))
```

Topics 2 and 3 look to be more prevalent in Taylor Swift's early albums, Topic 1 does look to be mostly from *Red*, and topic 13 is uncommon except in *folklore* and *evermore*.

## Estimate topic effects

There is a TON more you can do with topic models. For example, we can take the trained topic model and, using some supplementary metadata on our documents, estimate regressions for the **proportion** of each document about a topic with the metadata as the predictors. For example, let's estimate regressions for our topics with the album name as the predictor. This asks the statistical question, "Do the topics in Taylor Swift songs change across albums?" We looked at this question visually in the last section, but now we can build a model to look at it a different way.

```{r}
set.seed(123)

effects <-
  estimateEffect(
    1:13 ~ album_name,
    topic_model,
    taylor_album_songs |> distinct(track_name, album_name) |> arrange(track_name)
  )
```

You can use `summary(effects)` to see some results here, but you also can `tidy()` the output to be able to compute on it. Do we have evidence for any of the topics being related to album, in the sense of having a p-value less than 0.05?

```{r}
tidy(effects) |> 
    filter(term != "(Intercept)", p.value < 0.05)
```

Here we see evidence that there is more topic 11 from *reputation* and more topic 13 in both *folklore* and *evermore*. Certainly they are lyrically pretty distinct from her other work! What are some of the highest lift words for this topic?

```{r}
tidy(topic_model, matrix = "lift") |> 
  filter(topic == 13)
```


